---
title: "Season 5 Episode 11 | Predicting Loan Payback"
author: "Göktürk Çolak"
date: "2025-11-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Upload and Preprocessing

```{r}
# libraries
library(tidyverse)

# setting up the directory
setwd("Github/kaggle-playground-series/Season 5 Episode 11")

# uploading the files
train <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")

train <- train %>% 
  mutate(loan_paid_back = as.factor(loan_paid_back)) 

levels(train$loan_paid_back)
```

# Exploratory Data Analysis

```{r}
train %>% 
  ggplot( aes(x = credit_score, fill = loan_paid_back)) +
  geom_density(alpha = 0.5) +
  labs(title = "Credit Score Distribution by Payback Status")


train %>% 
  ggplot(aes(x = employment_status, fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(title = "Payback Rate by Employment Status", y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


train %>% 
  ggplot(aes(x = credit_score, fill = loan_paid_back)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~employment_status) +
  labs(title = "Credit Score Distribution by Employment Status and Payback")



# Note: Omit outliers since IQR is low (0.084) - DONE
# summary(train$debt_to_income_ratio)
Q1_dti <- quantile(train$debt_to_income_ratio, 0.25)
Q3_dti <- quantile(train$debt_to_income_ratio, 0.75)
IQR_dti <- IQR(train$debt_to_income_ratio)

lower_bound_dti <- Q1_dti - 1.5 * IQR_dti
upper_bound_dti <- Q3_dti + 1.5 * IQR_dti

# Filter out outliers
train_clean <- train %>%
  filter(debt_to_income_ratio >= lower_bound_dti & debt_to_income_ratio <= upper_bound_dti)


train_clean %>% 
  ggplot(aes(x = debt_to_income_ratio, fill = loan_paid_back)) +
  geom_density(alpha = 0.3) +
  labs(title = "Debt-to-Income Ratio by Payback Status (Outliers Removed)")


# ? Not sure, model may catch some signals
train %>% 
  ggplot(aes(x = loan_purpose, fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(title = "Payback Rate by Loan Purpose", y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# A1:B5 ~ Safe
# C1:C5 ~ Moderate
# D1:F5 ~ Dangerous
train %>% 
  ggplot(aes(x = grade_subgrade, fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(title = "Payback Rate by Loan Grade/Subgrade", y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# corrplot ---------------------------------------------------------------------
library(corrplot)

numerical_vars <- train %>% 
  select(annual_income, debt_to_income_ratio, credit_score, loan_amount, interest_rate, loan_paid_back) %>%
  mutate(loan_paid_back = as.numeric(as.character(loan_paid_back)))

cor_matrix <- cor(numerical_vars, use = "complete.obs")

corrplot(cor_matrix, method = "color", type = "upper")

# corrplot ends ----------------------------------------------------------------

# Interest Rate
Q1_interest <- quantile(train$interest_rate, 0.25)
Q3_interest <- quantile(train$interest_rate, 0.75)
IQR_interest <- IQR(train$interest_rate)

lower_bound_interest <- Q1_interest - 1.5 * IQR_interest
upper_bound_interest <- Q3_interest + 1.5 * IQR_interest

train_clean <- train %>%
  filter(interest_rate >= lower_bound_interest & interest_rate <= upper_bound_interest)

train_clean %>% 
  ggplot(aes(interest_rate, fill = loan_paid_back)) +
  geom_density(alpha = .3)
```

--------------------------------------------------------------------------------

EDAs tested: 
1 - loan amount X interest rate = no
2 - interest X target = no
3 - gender X target = no
4 - education X target = no
5 - annual income X target = no

--------------------------------------------------------------------------------

# Feature Engineering

```{r}
# 1. Credit Score Binning - Done
train <- train %>%
  mutate(credit_risk_tier = case_when(credit_score <= 650 ~ "High_Risk",
                                      credit_score > 650 & credit_score < 700 ~ "Medium_Risk", 
                                      credit_score >= 700 ~ "Low_Risk")
         )

# 2. Employment Stability - ?
train <- train %>%
  mutate(employment_stability = case_when(employment_status %in% c("Employed", "Retired", "Self-employed") ~ "Stable",
                                          employment_status == "Student" ~ "Moderate", 
                                          employment_status == "Unemployed" ~ "Unstable"))

# 3. High-Risk Combination - ?
train <- train %>%
  mutate(high_risk_combination = ifelse(employment_status %in% c("Retired", "Student") & credit_score <= 650, 1, 0))

# Target encoding for employment (if you have enough data) - ? (get it explained by Gemini/Deepseek)
employment_payback_rate <- train %>%
  group_by(employment_status) %>%
  summarise(emp_payback_rate = mean(as.numeric(as.character(loan_paid_back))))

train <- train %>% left_join(employment_payback_rate, by = "employment_status")


# 4. Subgrade Classification
train <- train %>%
  mutate(grade_subgrade = factor(grade_subgrade, levels = c(paste0("A", 1:5), paste0("B", 1:5),
                                                            paste0("C", 1:5), paste0("D", 1:5),
                                                            paste0("E", 1:5), paste0("F", 1:5)),
                                 ordered = TRUE),
         loan_risk_tier = case_when(grade_subgrade %in% c(paste0("A", 1:5), paste0("B", 1:5)) ~ "Safe",
                                    grade_subgrade %in% paste0("C", 1:5) ~ "Moderate",
                                    TRUE ~ "Dangerous"))

train <- train %>%
  mutate(interest_rate_risk = case_when(interest_rate < 12.5 ~ "Low_Risk",
                                        interest_rate >= 12.5 ~ "High_Risk"))
```

# Feature Plots

```{r}
# Check separation between risk tiers
ggplot(train, aes(x = credit_risk_tier, fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(title = "Payback Rate by Credit Risk Tier", 
       y = "Proportion", x = "Credit Risk Tier") +
  scale_fill_manual(values = c("red", "green"))



# Compare original vs engineered feature
p1 <- ggplot(train, aes(x = employment_status, fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(title = "Original Employment Status", y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- ggplot(train, aes(x = employment_stability, fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(title = "Engineered Employment Stability", y = "Proportion")

library(patchwork)
p1 + p2  # Side-by-side comparison


# Show the power of this flag
ggplot(train, aes(x = factor(high_risk_combination), fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(title = "Payback Rate by High-Risk Combination Flag",
       x = "High Risk Combination (Unemployed + Poor Credit)",
       y = "Proportion") +
  scale_x_discrete(labels = c("No", "Yes"))


# Show continuous relationship
ggplot(train, aes(x = emp_payback_rate, fill = loan_paid_back)) +
  geom_density(alpha = 0.7) +
  labs(title = "Distribution of Employment Payback Rate by Actual Outcome",
       x = "Employment Group Payback Rate",
       y = "Density") +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red")

```

# Combined Plots

```{r}
# Create a validation grid
library(patchwork)

plot1 <- ggplot(train, aes(x = credit_risk_tier, fill = loan_paid_back)) +
  geom_bar(position = "fill") + 
  theme(axis.text.x = element_text(angle = 45))

plot2 <- ggplot(train, aes(x = employment_stability, fill = loan_paid_back)) +
  geom_bar(position = "fill") + 
  theme(axis.text.x = element_text(angle = 45))

plot3 <- ggplot(train, aes(x = factor(high_risk_combination), fill = loan_paid_back)) +
  geom_bar(position = "fill") +
  labs(x = "High Risk Flag")

plot4 <- ggplot(train, aes(x = emp_payback_rate, color = loan_paid_back)) +
  geom_density() + 
  labs(x = "Emp Payback Rate")

(plot1 + plot2) / (plot3 + plot4) +
  plot_annotation(title = "Feature Engineering Validation")
```

# Enhanced Feature Engineering - FIXED
```{r}
# Load libraries
library(tidyverse)
library(catboost)
library(lightgbm)

# Enhanced feature engineering function - DEBUGGED
enhanced_feature_engineering <- function(df, train_df = NULL) {
  # Apply your original feature engineering
  df_engineered <- df %>%
    mutate(
      credit_risk_tier = factor(case_when(
        credit_score <= 650 ~ "High_Risk",
        credit_score > 650 & credit_score < 700 ~ "Medium_Risk", 
        credit_score >= 700 ~ "Low_Risk"
      )),
      
      employment_stability = factor(case_when(
        employment_status %in% c("Employed", "Retired", "Self-employed") ~ "Stable",
        employment_status == "Student" ~ "Moderate", 
        employment_status == "Unemployed" ~ "Unstable"
      )),
      
      high_risk_combination = as.factor(ifelse(
        employment_status %in% c("Retired", "Student") & credit_score <= 650, 1, 0
      )),
      
      loan_risk_tier = factor(case_when(
        grade_subgrade %in% c(paste0("A", 1:5), paste0("B", 1:5)) ~ "Safe",
        grade_subgrade %in% paste0("C", 1:5) ~ "Moderate",
        TRUE ~ "Dangerous"
      )),
      
      interest_rate_risk = factor(case_when(
        interest_rate < 12.5 ~ "Low_Risk",
        interest_rate >= 12.5 ~ "High_Risk"
      ))
    )
  
  # Target encoding for categorical variables (only if training data is provided)
  if (!is.null(train_df)) {
    # Convert loan_paid_back to numeric for calculations
    train_df <- train_df %>%
      mutate(loan_paid_back_num = as.numeric(as.character(loan_paid_back)))
    
    global_mean <- mean(train_df$loan_paid_back_num, na.rm = TRUE)
    
    # Employment encoding
    employment_encoding <- train_df %>%
      group_by(employment_status) %>%
      summarise(employment_encoded = mean(loan_paid_back_num, na.rm = TRUE))
    
    # Purpose encoding  
    purpose_encoding <- train_df %>%
      group_by(loan_purpose) %>%
      summarise(purpose_encoded = mean(loan_paid_back_num, na.rm = TRUE))
    
    # Grade encoding
    grade_encoding <- train_df %>%
      group_by(grade_subgrade) %>%
      summarise(grade_encoded = mean(loan_paid_back_num, na.rm = TRUE))
    
    # Debug: Check if encodings were created
    print("Employment encoding sample:")
    print(head(employment_encoding))
    
    # Merge encodings
    df_engineered <- df_engineered %>%
      left_join(employment_encoding, by = "employment_status") %>%
      left_join(purpose_encoding, by = "loan_purpose") %>%
      left_join(grade_encoding, by = "grade_subgrade")
    
    # Fill NA values with global mean
    df_engineered$employment_encoded <- ifelse(is.na(df_engineered$employment_encoded), global_mean, df_engineered$employment_encoded)
    df_engineered$purpose_encoded <- ifelse(is.na(df_engineered$purpose_encoded), global_mean, df_engineered$purpose_encoded)
    df_engineered$grade_encoded <- ifelse(is.na(df_engineered$grade_encoded), global_mean, df_engineered$grade_encoded)
  }
  
  return(df_engineered)
}

# Apply enhanced feature engineering
print("Applying feature engineering to train data...")
train_enhanced <- enhanced_feature_engineering(train)

print("Applying feature engineering to test data...")
test_enhanced <- enhanced_feature_engineering(test, train_df = train)

# Debug: Check what columns were created
print("Columns in train_enhanced:")
print(names(train_enhanced))

print("Columns in test_enhanced:")
print(names(test_enhanced))

# Check if encoded columns exist
print("Checking for encoded columns:")
print(paste("employment_encoded exists:", "employment_encoded" %in% names(train_enhanced)))
print(paste("purpose_encoded exists:", "purpose_encoded" %in% names(train_enhanced)))
print(paste("grade_encoded exists:", "grade_encoded" %in% names(train_enhanced)))

# Prepare features for different models - WITH SAFETY CHECKS
prepare_xgb_features <- function(df) {
  # Start with basic numerical features
  features <- df %>%
    select(
      credit_score, annual_income, debt_to_income_ratio, loan_amount, interest_rate,
      credit_risk_tier, employment_stability, high_risk_combination, loan_risk_tier, interest_rate_risk
    )
  
  # Add encoded features only if they exist
  if("employment_encoded" %in% names(df)) {
    features <- features %>% mutate(employment_encoded = df$employment_encoded)
  }
  if("purpose_encoded" %in% names(df)) {
    features <- features %>% mutate(purpose_encoded = df$purpose_encoded)
  }
  if("grade_encoded" %in% names(df)) {
    features <- features %>% mutate(grade_encoded = df$grade_encoded)
  }
  
  return(features)
}

prepare_catboost_features <- function(df) {
  # Start with basic numerical features
  features <- df %>%
    select(
      credit_score, annual_income, debt_to_income_ratio, loan_amount, interest_rate,
      employment_status, loan_purpose, grade_subgrade
    )
  
  # Add encoded features only if they exist
  if("employment_encoded" %in% names(df)) {
    features <- features %>% mutate(employment_encoded = df$employment_encoded)
  }
  if("purpose_encoded" %in% names(df)) {
    features <- features %>% mutate(purpose_encoded = df$purpose_encoded)
  }
  if("grade_encoded" %in% names(df)) {
    features <- features %>% mutate(grade_encoded = df$grade_encoded)
  }
  
  # Convert categorical columns to factors
  categorical_cols <- c("employment_status", "loan_purpose", "grade_subgrade")
  for(col in categorical_cols) {
    features[[col]] <- as.factor(features[[col]])
  }
  
  return(features)
}

# Prepare datasets
print("Preparing features for models...")
xgb_train_features <- prepare_xgb_features(train_enhanced)
xgb_test_features <- prepare_xgb_features(test_enhanced)
cb_train_features <- prepare_catboost_features(train_enhanced)
cb_test_features <- prepare_catboost_features(test_enhanced)

# Convert target
y_train <- as.numeric(as.character(train_enhanced$loan_paid_back))

print("Feature engineering completed successfully!")
print(paste("XGBoost train features:", ncol(xgb_train_features)))
print(paste("CatBoost train features:", ncol(cb_train_features)))
print(paste("Training samples:", nrow(train_enhanced)))
```

# XGBoost Model
```{r}
print("Training XGBoost with target encoding...")

# Prepare XGBoost data
xgb_features_matrix <- model.matrix(~ . - 1, data = xgb_train_features)
dtrain_xgb <- xgb.DMatrix(data = xgb_features_matrix, label = y_train)

xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "auc",
  eta = 0.05,
  max_depth = 8,
  min_child_weight = 3,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Cross-validation
xgb_cv <- xgb.cv(
  params = xgb_params,
  data = dtrain_xgb,
  nrounds = 1000,
  nfold = 5,
  early_stopping_rounds = 50,
  verbose = TRUE,
  print_every_n = 50
)

# Train final model
xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain_xgb,
  nrounds = xgb_cv$best_iteration,
  verbose = 1
)

print(paste("XGBoost training completed! Best CV AUC:", round(max(xgb_cv$evaluation_log$test_auc_mean), 5)))
```

# CatBoost Model
```{r}
print("Training CatBoost...")

# Convert to regular data frame
cb_train_df <- as.data.frame(cb_train_features)
cb_test_df <- as.data.frame(cb_test_features)

# Identify categorical columns (0-based index)
cat_features <- which(sapply(cb_train_df, is.factor)) - 1
print(paste("Categorical features at indices:", paste(cat_features, collapse = ", ")))

# Create CatBoost pools
train_pool <- catboost.load_pool(data = cb_train_df, label = y_train, cat_features = cat_features)
test_pool <- catboost.load_pool(data = cb_test_df, cat_features = cat_features)

# CatBoost parameters
catboost_params <- list(
  iterations = 500,
  learning_rate = 0.05,
  depth = 6,
  loss_function = "Logloss",
  eval_metric = "AUC",
  early_stopping_rounds = 20,
  verbose = 100
)

# Train CatBoost
catboost_model <- catboost.train(train_pool, NULL, params = catboost_params)

print("CatBoost training completed!")
```

# LightGBM Model
```{r}
print("Training LightGBM...")

# Prepare LightGBM data
lgb_features_matrix <- as.matrix(xgb_train_features)
lgb_train <- lgb.Dataset(data = lgb_features_matrix, label = y_train)

lgb_params <- list(
  objective = "binary",
  metric = "auc",
  learning_rate = 0.05,
  num_leaves = 31,
  max_depth = -1,
  min_data_in_leaf = 20
)

# Train LightGBM
lgb_model <- lgb.train(
  params = lgb_params,
  data = lgb_train,
  nrounds = 500,
  verbose = 1
)

print("LightGBM training completed!")
```

# Generate Predictions and Create Submissions
```{r}
print("Generating predictions and creating submission files...")

# XGBoost predictions
xgb_test_matrix <- model.matrix(~ . - 1, data = xgb_test_features)
dtest_xgb <- xgb.DMatrix(data = xgb_test_matrix)
xgb_preds <- predict(xgb_model, newdata = dtest_xgb)

# CatBoost predictions
catboost_preds <- catboost.predict(catboost_model, test_pool, prediction_type = "Probability")

# LightGBM predictions
lgb_test_matrix <- as.matrix(xgb_test_features)
lgb_preds <- predict(lgb_model, lgb_test_matrix)

# Create ensemble predictions
ensemble_preds <- 0.6 * xgb_preds + 0.3 * catboost_preds + 0.1 * lgb_preds

# Create submission files
submission_files <- list()

submission_files$xgb_enhanced <- data.frame(
  id = test_enhanced$id,
  loan_paid_back = xgb_preds
)

submission_files$catboost <- data.frame(
  id = test_enhanced$id,
  loan_paid_back = catboost_preds
)

submission_files$lgb <- data.frame(
  id = test_enhanced$id,
  loan_paid_back = lgb_preds
)

submission_files$ensemble_weighted <- data.frame(
  id = test_enhanced$id,
  loan_paid_back = ensemble_preds
)

# Equal weight ensemble
submission_files$ensemble_equal <- data.frame(
  id = test_enhanced$id,
  loan_paid_back = (xgb_preds + catboost_preds + lgb_preds) / 3
)

# Write all submission files
for (name in names(submission_files)) {
  filename <- paste0("submission_", name, ".csv")
  write.csv(submission_files[[name]], filename, row.names = FALSE)
  print(paste("Created:", filename))
}

print("=== SUBMISSION FILES CREATED ===")
print("Files created:")
print(paste("- submission_xgb_enhanced.csv (XGBoost only)"))
print(paste("- submission_catboost.csv (CatBoost only)"))
print(paste("- submission_lgb.csv (LightGBM only)"))
print(paste("- submission_ensemble_weighted.csv (Recommended: 60% XGB, 30% CatBoost, 10% LGB)"))
print(paste("- submission_ensemble_equal.csv (Equal weights)"))
print("=== COMPLETED SUCCESSFULLY ===")
```

